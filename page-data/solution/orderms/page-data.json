{"componentChunkName":"component---src-pages-solution-orderms-index-mdx","path":"/solution/orderms/","result":{"pageContext":{"frontmatter":{"title":"Vaccine Order Management","description":"This microservice manage order for vaccine"},"relativePagePath":"/solution/orderms/index.mdx","titleType":"append","MdxNode":{"id":"39e4d6d6-82c5-546a-9eee-48c68a82c870","children":[],"parent":"8d6b2b86-339a-5bf2-b9e2-7b84e98d8630","internal":{"content":"---\ntitle: Vaccine Order Management\ndescription: This microservice manage order for vaccine\n---\n\n\nThis microservice manages vaccine orders for a world wide demand and distribution. This is an example of using [Quarkus](https://quarkus.io), microprofile [Reactive Messaging](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/concepts.html), [Debezium outbox quarkus extension](https://debezium.io/documentation/reference/integrations/outbox.html) with Debezium [change data capture  for Postgresl](https://debezium.io/documentation/reference/connectors/postgresql.html) to Kafka. The database is Postgresql, the service uses Hibernate ORM with Panache. \n\nThe DevOps is supported by Git Action and then gitops repository.\n\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build and deploy to OpenShift</AnchorLink>\n  <AnchorLink>Demonstration script</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis project implements a very simple event driven microservice to support the Create, Read, Update, of vaccine orders. This implementation highlights the following capabilities and patterns:\n\n* Quarkus reactive microservice using Microprofile 3.x - reactive messaging extension to consume Vaccine lot shipment plans\n* Postgresql with Hibernate ORM with [Quarkus Panache](https://quarkus.io/guides/hibernate-orm-panache)\n* Quarkus Debezium Outbox pattern to get create OrderEvents while writing to the main VaccineOrderEntity table\n* Integrate a Kafka Connector with Debezium change data capture table updates on the OrderEvents table.\n\nIn term of business scenario, a sale representative uses his mobile device or web browser to enter information about a vaccine order to be shipped to a given country or a province within a country at given date for a given quantity: \n\n ![](./images/order-ui.png)\n\nThe user interface is for demonstration purpose only, and illustrates some standard Vuejs / vuetify components. \n\nThe following diagram presents the simple context view of the service deployed on OpenShift, integrated with Kafka, Change data capture and the [Vaccine order reefer optimization service](/solution/orderms/voro-solution/). The integration with this component is using pub/sub via Kafka topics. The integration with Blockchain is not done yet, but it can be seen as the source of truth for the business companies participating to then end to end delivery.  \n\n ![Deployment view](./images/vaccine-order-1.png)\n\n\nThe component writes to the database all the orders received, but also produces records to Kafka via the Outbox pattern and change data capture.\n\nAnother view could be a pods view of the solution to deploy. The Debezium change data capture is a Kafka connector.\n\n ![Pods view](./images/pods-map.png)\n\n**Github repository:** [Vaccine-order-mgr-pg](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n\n\n**Kafka topics produced to:** vaccine.public.vaccineorderentity\n**Kafka topics consumed from :** vaccine-shipment-plans\n\n**Events produced:**\n\nWe will simplify the process and aggregate in the following event types:\n\n* orderCreated\n* orderUpdated\n\n\n## Build and deploy to OpenShift\n\nThis microservice is built using maven and Quarkus extensions. We have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\nTo build and run locally see the [repository main readme](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg) as we have different docker-compose files to run in demonstration mode or in development mode.\n\nIn this section we address how to use OpenShift Source to Image to build and deploy the application to OpenShift. The application is using environment variables to access to user, password, URLs to access Postgres and Kafka. We are using Event Streams deployed in Cloud Pak for Integration, but it could work the same with Strimzi. \n\nAs an example we created the OpenShift project called \"vaccine\" with a command: `oc new-project vaccine`.\n\n### Pre requisites\n\n* Clone the source git repository: `git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg`\n* Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n* Get an instance of Kafka deployed on OpenShift like [IBM Event Streams](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#install-event-streams-using-operators)...). \n\n\n### Define Kafka user and postgres user secrets\n\n* Get Kafka user to access Event Streams bootstrap in external route.\n\n ```shell\n oc get kafkausers -n eventstreams \n # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n # app-scram                           eda-dev   scram-sha-512    simple\n # app-tls                             eda-dev   tls              simple\n ```\n* Get password with: ``\n* Define a secret manifest with a command like below, or by updating the  [src/main/kubernetes/secrets.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/kubernetes/secrets.yaml) secret file.\n\n ```shell\n oc apply -f - <<EOF\n apiVersion: v1\n kind: Secret\n metadata:\n   name: vaccine-order-secrets\n data:\n    KAFKA_USER: <username-base64-encoded>\n    KAFKA_PASSWORD: <pwd-base64-encoded>\n    QUARKUS_DATASOURCE_PASSWORD: <postgres-user-pwd-base64-encoded>\n    QUARKUS_DATASOURCE_USERNAME: <postgres-user-base64-encoded>\n EOF\n ```\n\nThe strings in the secret are base64 encoded, so use something like: `echo \"app-scram\" | base64 `. The password is already encrypted in a secret within the `eventstreams` project, so use `oc get secret app-scram -n eventstreams -o jsonpath='{.data.password}'` command.\n\n\n### Get Event Streams certificates\n\nFor Event Streams get URL of the bootstrap server and the service credentials (TLS certificates) following [those instructions.](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift)\n\nIf not done before, copy the cluster certificate via the secret to your project (`vaccine`):\n\n  ```shell\n  oc get secret eda-dev-cluster-ca-cert -n eventstreams --export -o yaml | oc apply -f -\n  ```\n\n### Define environment variables via ConfigMap\n\nModify the config map from [src/main/kubernetes/configmap.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/kubernetes/configmap.yaml) with the URL of your Kafka bootstrap server.\n\n  ```yaml\n  apiVersion: v1\n  kind: ConfigMap\n  metadata:\n    name: vaccine-order-ms-cm\n  data:\n    KAFKA_BOOTSTRAP_SERVERS: \"light-es-kafka-bootstrap.eventstreams.svc:9092\"\n    KAFKA_SSL_PROTOCOL: \"TLSv1.2\"\n    KAFKA_SSL_TRUSTSTORE_LOCATION: \"/deployments/certs/server/ca.p12\"\n    KAFKA_SSL_TRUSTSTORE_TYPE: \"PKCS12\"\n    SHIPMENT_PLAN_TOPIC: \"vaccine_shipment_plans\"\n    KAFKA_SASL_MECHANISM: SCRAM-SHA-512\n    KAFKA_SECURITY_PROTOCOL: SASL_SSL\n  ```\n\nThose environment variables are used by the application, and configured via the [src/main/resources/application.properties](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/resources/application.properties). Below is the declaration that defines environment variables from the previously created secrets and configmap.\n\n  ```properties\n  quarkus.openshift.env.configmaps=vaccine-order-ms-cm\n  quarkus.openshift.env.secrets=vaccine-order-secrets\n  quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=eda.dev-cluster-ca-cert\n  quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n  quarkus.openshift.mounts.es-cert.path=/deployments/certs/server\n  quarkus.openshift.secret-volumes.es-cert.secret-name=eda-gse-cluster-ca-cert\n  ```\n\n* Then proceed with the following command to define the config map into OpenShift:\n\n ```shell\n oc apply -f src/main/kubernetes/configmap.yaml\n ```\n\n### Deploy the application\n\nThe application uses Quarkus OpenShift extension to create yaml files for OpenShift and deploy the application using source to image capability of OpenShift:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\nThe `-Dui.deps -Dui.dev` arguments are used to prepare and build the vue.js app from the `ui` folder. The packaging build a runner jar and push it to the private image registry in OpenShift.\n\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we set properties to expose the application, an OpenShift route was created. \n\n* Be sure to get the Order Microservice URL to access the user interface, using `oc get routes` on the project. Using your web browser, pointing to this address you should get the UI:\n\n ![](./images/ui-home.png)\n\n\n### Deploy Debezium CDC connector\n\nThe Event Streams product documentation goes over the tasks to be done, but to summarize them, here are the actions done to deploy the postgres debezium connector:\n\n* Start a Kafka connector cluster: We use the custom resource called `KafkaConnectS2I`, which one instance represent a Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. Kafka connect needs the same user access to the Brokers. \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Modify the bootstrap server and the TLS user to be used. Ensure the user can access any topics.\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n eventstreams`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n eventstreams\n   oc describe kafkaconnects2i connect-cluster -n eventstreams\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. \n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/\n  oc get builds\n  # NAME                        TYPE     FROM             STATUS    STARTED          DURATION\n  # connect-cluster-connect-3   Source   Binary@f639186   Running   19 seconds ago   \n  # once build is completed... wait to get the 3 kafka connect workers ready\n  oc get pods -w\n  ```\n* Start the connector: `oc apply -f pg-connector.yaml`\n* Verify it is running: `oc describe kafkaconnector pg-connector` \n\n## Demonstration script\n\n* Connect to the order microservice URL, for example it could look like http://vaccineorderms-vaccine.clusternametochangewithyours.containers.appdomain.cloud. You should reach the home page. Then select Orders tab. \n\nIf there is no order created click on `NEW ORDER` button \n\n ![](./images/order-ui.png)\n\nand fill the following data:\n\n ```yaml\n organization: French Government\n delivery location: Paris\n delivery date: 2021-01-24\n quantity: 150\n priority: 2\n vaccine type: covid-19\n ```\n\n\n* Verify the data in postgresql: \n\nOnce submitted the data are saved into the postgres DB, with the outbox pattern having created new records in the corresponding Event tables. \nThis can be done using different tools, like `psql` directly in the pod, or `pgadmin4`.\n\n ```shell\n oc get pods | grep postgres\n oc rsh <postgres pod id>\n # in the shell session within the pod do:\n psql -U postgres\n # in the psql shell, list the table\n \\d \n # Look at the content of the main table:\n select * from public.vaccineorderentity;\n # You should get one new record matching your data\n # Verify outbox pattern works:\n select * from public.orderevents;\n\n id    |   aggregatetype    | aggregateid | type         | timestamp |payload \n c80.. | VaccineOrderEntity | 1           | OrderCreated | 2021-01-24 03:44:08.131634 | {\"orderID\":1,\"deliveryLocation\":\"Paris\",\"quantity\":150,\"priority\":2,\"deliveryDate\":\"2021-01-24\",\"askingOrganization\":\"French Government\",\"vaccineType\":\"COVID-19\",\"status\":\"OPEN\",\"creationDate\":\"24-Jan-2021 03:44:08\"} | \n ```\n\nThe outbox table of the order events has metadata attributes and then a payload matches the saved record in the orign table.\n\n* Verify the message in the Kafka topic: `vaccine.public.orderevents`\n\n ![](./images/outbox-topic.png)\n\n  ```json\n  {\"before\":null,\"after\":\n  {\"id\":\"c8050bb4-05d8-4270-833c-083995f27848\",\n   \"aggregatetype\":\"VaccineOrderEntity\",\n   \"aggregateid\":\"1\",\n   \"type\":\"OrderCreated\",\n   \"timestamp\":1611459848131634,\n   \"payload\":\"{\\\"orderID\\\":1,\\\"deliveryLocation\\\":\\\"Paris\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2021-01-24\\\",\\\"askingOrganization\\\":\\\"French Government\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"24-Jan-2021 03:44:08\\\"}\",\n  \"tracingspancontext\":\"#Sun Jan 24 03:44:08 GMT 2021\\n\"},\n  \"source\":{\"version\":\"1.4.0.Final\",\n  \"connector\":\"postgresql\",\"name\":\"vaccine\",\n  \"ts_ms\":1611712125168,\"snapshot\":\"last\",\n  \"db\":\"postgres\",\"schema\":\"public\",\"table\":\"orderevents\",\n  \"txId\":1371,\"lsn\":41341808,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1611712125173,\"transaction\":null}\n  ```\n\n### REST APIs\n\nThe REST end points this service expose are in the OpenApi doc below, but not all operations are fully implemented yet.\n\n ![4](./images/openapi.png)\n\n","type":"Mdx","contentDigest":"295bbba813808a4a4939cf18f02334b8","counter":254,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Vaccine Order Management","description":"This microservice manage order for vaccine"},"exports":{},"rawBody":"---\ntitle: Vaccine Order Management\ndescription: This microservice manage order for vaccine\n---\n\n\nThis microservice manages vaccine orders for a world wide demand and distribution. This is an example of using [Quarkus](https://quarkus.io), microprofile [Reactive Messaging](https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2/concepts.html), [Debezium outbox quarkus extension](https://debezium.io/documentation/reference/integrations/outbox.html) with Debezium [change data capture  for Postgresl](https://debezium.io/documentation/reference/connectors/postgresql.html) to Kafka. The database is Postgresql, the service uses Hibernate ORM with Panache. \n\nThe DevOps is supported by Git Action and then gitops repository.\n\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build and deploy to OpenShift</AnchorLink>\n  <AnchorLink>Demonstration script</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis project implements a very simple event driven microservice to support the Create, Read, Update, of vaccine orders. This implementation highlights the following capabilities and patterns:\n\n* Quarkus reactive microservice using Microprofile 3.x - reactive messaging extension to consume Vaccine lot shipment plans\n* Postgresql with Hibernate ORM with [Quarkus Panache](https://quarkus.io/guides/hibernate-orm-panache)\n* Quarkus Debezium Outbox pattern to get create OrderEvents while writing to the main VaccineOrderEntity table\n* Integrate a Kafka Connector with Debezium change data capture table updates on the OrderEvents table.\n\nIn term of business scenario, a sale representative uses his mobile device or web browser to enter information about a vaccine order to be shipped to a given country or a province within a country at given date for a given quantity: \n\n ![](./images/order-ui.png)\n\nThe user interface is for demonstration purpose only, and illustrates some standard Vuejs / vuetify components. \n\nThe following diagram presents the simple context view of the service deployed on OpenShift, integrated with Kafka, Change data capture and the [Vaccine order reefer optimization service](/solution/orderms/voro-solution/). The integration with this component is using pub/sub via Kafka topics. The integration with Blockchain is not done yet, but it can be seen as the source of truth for the business companies participating to then end to end delivery.  \n\n ![Deployment view](./images/vaccine-order-1.png)\n\n\nThe component writes to the database all the orders received, but also produces records to Kafka via the Outbox pattern and change data capture.\n\nAnother view could be a pods view of the solution to deploy. The Debezium change data capture is a Kafka connector.\n\n ![Pods view](./images/pods-map.png)\n\n**Github repository:** [Vaccine-order-mgr-pg](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n\n\n**Kafka topics produced to:** vaccine.public.vaccineorderentity\n**Kafka topics consumed from :** vaccine-shipment-plans\n\n**Events produced:**\n\nWe will simplify the process and aggregate in the following event types:\n\n* orderCreated\n* orderUpdated\n\n\n## Build and deploy to OpenShift\n\nThis microservice is built using maven and Quarkus extensions. We have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\nTo build and run locally see the [repository main readme](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg) as we have different docker-compose files to run in demonstration mode or in development mode.\n\nIn this section we address how to use OpenShift Source to Image to build and deploy the application to OpenShift. The application is using environment variables to access to user, password, URLs to access Postgres and Kafka. We are using Event Streams deployed in Cloud Pak for Integration, but it could work the same with Strimzi. \n\nAs an example we created the OpenShift project called \"vaccine\" with a command: `oc new-project vaccine`.\n\n### Pre requisites\n\n* Clone the source git repository: `git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg`\n* Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n* Get an instance of Kafka deployed on OpenShift like [IBM Event Streams](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#install-event-streams-using-operators)...). \n\n\n### Define Kafka user and postgres user secrets\n\n* Get Kafka user to access Event Streams bootstrap in external route.\n\n ```shell\n oc get kafkausers -n eventstreams \n # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n # app-scram                           eda-dev   scram-sha-512    simple\n # app-tls                             eda-dev   tls              simple\n ```\n* Get password with: ``\n* Define a secret manifest with a command like below, or by updating the  [src/main/kubernetes/secrets.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/kubernetes/secrets.yaml) secret file.\n\n ```shell\n oc apply -f - <<EOF\n apiVersion: v1\n kind: Secret\n metadata:\n   name: vaccine-order-secrets\n data:\n    KAFKA_USER: <username-base64-encoded>\n    KAFKA_PASSWORD: <pwd-base64-encoded>\n    QUARKUS_DATASOURCE_PASSWORD: <postgres-user-pwd-base64-encoded>\n    QUARKUS_DATASOURCE_USERNAME: <postgres-user-base64-encoded>\n EOF\n ```\n\nThe strings in the secret are base64 encoded, so use something like: `echo \"app-scram\" | base64 `. The password is already encrypted in a secret within the `eventstreams` project, so use `oc get secret app-scram -n eventstreams -o jsonpath='{.data.password}'` command.\n\n\n### Get Event Streams certificates\n\nFor Event Streams get URL of the bootstrap server and the service credentials (TLS certificates) following [those instructions.](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift)\n\nIf not done before, copy the cluster certificate via the secret to your project (`vaccine`):\n\n  ```shell\n  oc get secret eda-dev-cluster-ca-cert -n eventstreams --export -o yaml | oc apply -f -\n  ```\n\n### Define environment variables via ConfigMap\n\nModify the config map from [src/main/kubernetes/configmap.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/kubernetes/configmap.yaml) with the URL of your Kafka bootstrap server.\n\n  ```yaml\n  apiVersion: v1\n  kind: ConfigMap\n  metadata:\n    name: vaccine-order-ms-cm\n  data:\n    KAFKA_BOOTSTRAP_SERVERS: \"light-es-kafka-bootstrap.eventstreams.svc:9092\"\n    KAFKA_SSL_PROTOCOL: \"TLSv1.2\"\n    KAFKA_SSL_TRUSTSTORE_LOCATION: \"/deployments/certs/server/ca.p12\"\n    KAFKA_SSL_TRUSTSTORE_TYPE: \"PKCS12\"\n    SHIPMENT_PLAN_TOPIC: \"vaccine_shipment_plans\"\n    KAFKA_SASL_MECHANISM: SCRAM-SHA-512\n    KAFKA_SECURITY_PROTOCOL: SASL_SSL\n  ```\n\nThose environment variables are used by the application, and configured via the [src/main/resources/application.properties](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/master/src/main/resources/application.properties). Below is the declaration that defines environment variables from the previously created secrets and configmap.\n\n  ```properties\n  quarkus.openshift.env.configmaps=vaccine-order-ms-cm\n  quarkus.openshift.env.secrets=vaccine-order-secrets\n  quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=eda.dev-cluster-ca-cert\n  quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n  quarkus.openshift.mounts.es-cert.path=/deployments/certs/server\n  quarkus.openshift.secret-volumes.es-cert.secret-name=eda-gse-cluster-ca-cert\n  ```\n\n* Then proceed with the following command to define the config map into OpenShift:\n\n ```shell\n oc apply -f src/main/kubernetes/configmap.yaml\n ```\n\n### Deploy the application\n\nThe application uses Quarkus OpenShift extension to create yaml files for OpenShift and deploy the application using source to image capability of OpenShift:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\nThe `-Dui.deps -Dui.dev` arguments are used to prepare and build the vue.js app from the `ui` folder. The packaging build a runner jar and push it to the private image registry in OpenShift.\n\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we set properties to expose the application, an OpenShift route was created. \n\n* Be sure to get the Order Microservice URL to access the user interface, using `oc get routes` on the project. Using your web browser, pointing to this address you should get the UI:\n\n ![](./images/ui-home.png)\n\n\n### Deploy Debezium CDC connector\n\nThe Event Streams product documentation goes over the tasks to be done, but to summarize them, here are the actions done to deploy the postgres debezium connector:\n\n* Start a Kafka connector cluster: We use the custom resource called `KafkaConnectS2I`, which one instance represent a Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. Kafka connect needs the same user access to the Brokers. \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Modify the bootstrap server and the TLS user to be used. Ensure the user can access any topics.\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n eventstreams`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n eventstreams\n   oc describe kafkaconnects2i connect-cluster -n eventstreams\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. \n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/\n  oc get builds\n  # NAME                        TYPE     FROM             STATUS    STARTED          DURATION\n  # connect-cluster-connect-3   Source   Binary@f639186   Running   19 seconds ago   \n  # once build is completed... wait to get the 3 kafka connect workers ready\n  oc get pods -w\n  ```\n* Start the connector: `oc apply -f pg-connector.yaml`\n* Verify it is running: `oc describe kafkaconnector pg-connector` \n\n## Demonstration script\n\n* Connect to the order microservice URL, for example it could look like http://vaccineorderms-vaccine.clusternametochangewithyours.containers.appdomain.cloud. You should reach the home page. Then select Orders tab. \n\nIf there is no order created click on `NEW ORDER` button \n\n ![](./images/order-ui.png)\n\nand fill the following data:\n\n ```yaml\n organization: French Government\n delivery location: Paris\n delivery date: 2021-01-24\n quantity: 150\n priority: 2\n vaccine type: covid-19\n ```\n\n\n* Verify the data in postgresql: \n\nOnce submitted the data are saved into the postgres DB, with the outbox pattern having created new records in the corresponding Event tables. \nThis can be done using different tools, like `psql` directly in the pod, or `pgadmin4`.\n\n ```shell\n oc get pods | grep postgres\n oc rsh <postgres pod id>\n # in the shell session within the pod do:\n psql -U postgres\n # in the psql shell, list the table\n \\d \n # Look at the content of the main table:\n select * from public.vaccineorderentity;\n # You should get one new record matching your data\n # Verify outbox pattern works:\n select * from public.orderevents;\n\n id    |   aggregatetype    | aggregateid | type         | timestamp |payload \n c80.. | VaccineOrderEntity | 1           | OrderCreated | 2021-01-24 03:44:08.131634 | {\"orderID\":1,\"deliveryLocation\":\"Paris\",\"quantity\":150,\"priority\":2,\"deliveryDate\":\"2021-01-24\",\"askingOrganization\":\"French Government\",\"vaccineType\":\"COVID-19\",\"status\":\"OPEN\",\"creationDate\":\"24-Jan-2021 03:44:08\"} | \n ```\n\nThe outbox table of the order events has metadata attributes and then a payload matches the saved record in the orign table.\n\n* Verify the message in the Kafka topic: `vaccine.public.orderevents`\n\n ![](./images/outbox-topic.png)\n\n  ```json\n  {\"before\":null,\"after\":\n  {\"id\":\"c8050bb4-05d8-4270-833c-083995f27848\",\n   \"aggregatetype\":\"VaccineOrderEntity\",\n   \"aggregateid\":\"1\",\n   \"type\":\"OrderCreated\",\n   \"timestamp\":1611459848131634,\n   \"payload\":\"{\\\"orderID\\\":1,\\\"deliveryLocation\\\":\\\"Paris\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2021-01-24\\\",\\\"askingOrganization\\\":\\\"French Government\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"24-Jan-2021 03:44:08\\\"}\",\n  \"tracingspancontext\":\"#Sun Jan 24 03:44:08 GMT 2021\\n\"},\n  \"source\":{\"version\":\"1.4.0.Final\",\n  \"connector\":\"postgresql\",\"name\":\"vaccine\",\n  \"ts_ms\":1611712125168,\"snapshot\":\"last\",\n  \"db\":\"postgres\",\"schema\":\"public\",\"table\":\"orderevents\",\n  \"txId\":1371,\"lsn\":41341808,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1611712125173,\"transaction\":null}\n  ```\n\n### REST APIs\n\nThe REST end points this service expose are in the OpenApi doc below, but not all operations are fully implemented yet.\n\n ![4](./images/openapi.png)\n\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/solution/orderms/index.mdx"}}}}